CURSO KAFKA
SISTEMA PRODUTO NAO FICA SOBRECARREGDO, POIS EM VE DE TODOS ACESSAREM O SISTEMA, ELES ACESSA AS FILAS

----INTRODUÇÃO
		CLUESTER KAFKA - VARIS SERVIDORES 
		BROKER - SERVIDORES COM DIFERENTES TOPICOS E DIFERENTES PARTICOES
		BOOTSTRAP - DIRECIONA O CONSUMIDOR PARA APENAS UM BROKER, POIS ELE SABE A ESTRUTURA 
		TOPICO PODEM TER VARIAS PARTICOES

		FOLLOWER - PARTIÇÕES LIDER QUE SÃO DISTRIBUIDAS EM OUTROS BORKWES PARA REPLICA EM CASO DE FALHA
		ISR - LISTA COM AS PARTIÇÕES/REPLICAS QUE ESTÃO SINCRONIZADOS, POIS SE UMA FALAHR OUTRA ASSUME COMO LEADER, FLOLLOWER

		COMMITED, UNCOMMITED
		MENSAGENS CONFIRMADAS E NAO CONFIRMADAS
		SE PERDEMOS (CAIR) O LIDER E NÃO TIVERMOS UMA REPLICA, SÓ PERDEMOS AS MENSAGENS NAO CONFIRMADAS

		MINIMUM ISR LIST - NUMERO MINIMO DE REPLICAS NA LISTA
		SE NAO HOUVER O MINIMO O BROKER NAO ACEITA REPLICAS

		SEGMENTOS - OS ARQUIVOS DE PARTIÇÕES SÃO DIVIDIDOS EM ARQUIVOS MENORES CHAMADOS DE SEGMENTOS

		OFFSET - NUMERO CHAVE DA MENSAGEM NA PARTIÇÃO
		SE TEMOS 4 PARTIÇÕES, TODA PARTIÇÃO VAI COMEÇAR COM OFFSET ZERO
		 
		ARQUIVO TIMEINDEX - BUSCAR MENSAGEM BASEADAS EM INTERVALO DE TEMPO
		
		ZOOKEPER - GESTÃO DE RECURSOS DE SISTEMAS DISTRIBUIDO, UTILIZADO PARA O KAFKA
		
		TOPICOS - ASSUNTOS E SÃO IMUTÁVEIS, SEMELHANTE A UMA TABELA DE BANCO DE DADOS, A RETENÇÃP DAS MENSAGENS POR PADRAO É 7 DIAS, MAS É CONFIGURAVEL
		
		OFFSET - IDENTIFICADOR UNICO DE UMA MENSAGEM NA PARTIÇÃO
		OFFSET SAO COMITADOS NUM TOPICO CHAMADO __consumer_offsets. O CONSUMIDOR PODE LER NOVAMENTE A PARTIRE DO NUMERO DO OFFSET
		
		IDENTIFICANDO A MENSAGEM DE FORMA UNICA - PRECISAMOS SABER O NOME DO TOPICO, NUMERO DA PARTIÇÃO E O OFFSET
		NORMALMENTE O CONSUMIDOR PROCESSA MENSAGENS EM INTERVALOS DE OFFSET
		
		ELEMENTPS DE UMA MENSAGEM
		OBRIGATORIO - TOPICO, MENSAGEM
		OPCIONAL - PARTIÇÃO, HASHKEY, TIMESTAMP, MESSAGE KEY (USADO PARA PARTICIONAMENTO, JOINS, AGRUPAMENTPOS)
		
		SERIALIZAÇÃO - DADOS SAO SERIALZIADOS PARA TRNASFERIR NA REDE. PODEM SER SERIALZIADOS COMO AVRO
		
		PRODUCERS - UTILZIA A API DO KAFKA PRODUCERRECORD PARA INPOUT DE MENSGAEM
		MANSAGENS PODE TER UMA KEY QUE GARANTE QUE MSGS COM A MESMA CHAVE VAO PARA A MESMA PARTICAO, ENQUANTO O NMERO DE PARTIÇÃO NAO MUDAR (FUNÇÃO HASH)
		VOCE PODE DEFINIR SUA PROPRIA LOGICA DE PARTICIONAMENTO
		
		COMO CRIAR PRODUCER/CONSUMER - VIA CONSOLE DO KAFKA, API OU KAFKA CONNECT
		
		CONSUMER - LE DADOS DE UMA OU MAIS PARTIÇÕES. OS DADOS SAO LIDOS EM ORDEM E NÃO EXISTE GARANTIA DE ORDEM ENTRE AS PARTIÇÕES
		
		CONBSUMER GROUP - UM CONSUMIDOR DE GRUPO PODE CONSUMIR VARIAS PARTIÇÕES/REPLICAS
		
		GARANTIA DE ENTREGA CONFIRMAÇÃO(AKNOWLEGEMENTS) - PARAMETRO ACKS 
		- PARAMETRO(0) - PRODUTOR CONSIDERA MSG RECEBIDA COM SUCESSO
		- PARAMETRO(1) - PRODUTOR CONSIDERA MSG RECEBIDA COM SUCESSO SE CONFIRMADA PELA PARTIÇÃO LEADER
		- PARAMETRO(ALL) - PRODUTOR CONSIDERA MSG RECEBIDA COM SUCESSO SE TODAS REPLICAS MINIMAS REECBEREM AS MENSAGENS
		CENARIOS QUE PODEM SER CONFIGURADOS:
		- O PRODUTOR ENVIA PELO MENOS UMA VEZ , SE HOIUVER FALAHA OU NAO CONFIRMACAO O PRODUTOE REENVIA E SE O BROKER RECEBER A MSG MAIS DE UMA VEZ ELE VAI ENTREGAR MAIS DE UMA VEZ
		- O PRODUTOS ENVIA A MSG SÓ UMA VEZ E NAO VAI REENVIAR A MSG SE TIVER FALHA OU NAO CONFIRMACAO. NESTE CASO SE TIVER DUPLICIDADE O BRKER ENVIA A MSG SOMENTE UMA VEZ
		
		COMO PRODURIZ/CONSUMIR DADOS - VIA CONSOLE, VIA API OU KAFKA CONNECT
		
		KAFKA CONNECT - CONECTORES PRONTOS, SÓ DECLARAR E PODE SER USADO COMO CONSUMIDOR E PRODUTOR, ESACAVEL AUTOMATICAMENTE, TOLERANTE A FALAHA, BALANCEA CARGA
		KAFKA CONNECT SOURCE - UTILIZADO PARA O PRODUTOR
		KAFKA CONNECT SINK - UTILIZADO PARA O CONSUMIDOR
		
		TRANSFORMACOES SMT - TECNICAS DISPONIVEIS PARA TRNAFORMAR DADOS
		
		
		
		
		
		
----PYTHON PRODUCER		
		from kafka import KafkaProducer as kp
		import ramdom
		
		#indica o servidor broker
		produtor = kp(bootstrap_servers='ip')
		for x in range(10):
			n = ramdom.ramdom()
			produto.send('nome_topico',key=x, value=n)
			
----PYTHON CONSUMER
		from kafka import KafkaConsumer as kc
		import ramdom
		
		#indica o servidor broker
		consumidor = kc('nome_topico', bootstrap_servers='ip', consumer_timeout_ms=1000, group_id='consumidor')
		for msg in consumidor:
			print('Topic:' + msg.topic)
			print('Partition:' + msg.partition)
			print('Chave:' + msg.key)
			print('OffSet (posicao):' + msg.offset)
			print('Msg:' + msg.value)
			
			
		
----KAFKA CONNECT
		INSTALAR ELASTIC SEARCH SINK CONNECTOR
		CRIAR DOIS ARQUIVOS DE CONFIGURAÇÃO
		1º CRIAR ARQUIVO DE CONF DO ELASTIC SEARCH
		2º ALTERAR DAS PROPRIEDADES DO STANDALONE
		
		get VIA ELASTIC SEARCH
		curl -XGET ip:porta/meuindice/_search?pretty
		
		
		
		
		



